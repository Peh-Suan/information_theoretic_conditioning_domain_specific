{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import glob\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "device = torch.device(\"mps\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_probability(context, word, model, tokenizer):\n",
    "    model.to(torch.device(\"mps\"))\n",
    "    # Tokenize the context and the word\n",
    "    context_tokens = tokenizer.encode(context, add_special_tokens=True)\n",
    "    word_tokens = tokenizer.encode(word, add_special_tokens=False)[1:]\n",
    "        \n",
    "    # Combine context with word tokens for probability calculation\n",
    "    combined_tokens = context_tokens + word_tokens\n",
    "        \n",
    "    total_prob = 1.0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(context_tokens), len(combined_tokens)):\n",
    "            # Get the input tokens up to the current token\n",
    "            input_tokens = combined_tokens[:i]\n",
    "            target_token = combined_tokens[i]\n",
    "                \n",
    "            # Convert to tensor\n",
    "            input_tensor = torch.tensor([input_tokens]).to(torch.device(\"mps\"))\n",
    "                \n",
    "            # Get model output and probabilities\n",
    "            outputs = model(input_tensor)\n",
    "            logits = outputs.logits  # (batch_size, seq_len, vocab_size)\n",
    "            last_token_logits = logits[0, -1, :]  # Get logits for the last token\n",
    "            probs = F.softmax(last_token_logits, dim=-1)\n",
    "\n",
    "            # Get the probability of the target token\n",
    "            token_prob = probs[target_token].item()\n",
    "            total_prob *= token_prob  # Multiply probabilities\n",
    "\n",
    "    return total_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"selected_sentences.pickle\", \"rb\") as f:\n",
    "    sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_lists = [\n",
    "    ([\"兩造\",], [\"雙方\"]),\n",
    "    ([\"上揭\", \"上開\", \"前揭\", \"前開\", \"首揭\"], [\"前述\", \"上述\"]),\n",
    "    ([\"云云\",], [\"等陳述\", \"等語\", \"等等\"]),\n",
    "    ([\"可考\", \"可佐\", \"可按\", \"可稽\", \"可證\", \"足按\", \"足徵\", \"足稽\", \"足憑\", \"足證\"], [\"可以佐證\", \"可供證明\", \"可以證明\", \"足以佐證\", \"足以證明\"]),\n",
    "    ([\"迭\",], [\"接連\", \"多次\"]),\n",
    "    ([\"拘束\",], [\"限制\",]),\n",
    "    ([\"失所附麗\",], [\"失所依附\",]),\n",
    "    ([\"尚非無稽\", \"尚非無憑\", \"尚非無據\", \"尚非虛妄\", \"尚非臆造\"], [\"應可採信\", \"應屬事實\", \"並非全無依據\", \"並不是完全沒有依據\"]),\n",
    "    ([\"所載\",], [\"所記載\",]),\n",
    "    ([\"考諸\", \"徵諸\", \"觀諸\", \"稽之\"], [\"參考\", \"依照\", \"依據\"]),\n",
    "    ([\"質言之\",], [\"簡言之\",]),\n",
    "    ([\"相歧\",], [\"矛盾\",]),\n",
    "    ([\"即非法所不許\", \"依法即無不合\",], [\"符合法律規定\",]),\n",
    "    ([\"礙難採認\",], [\"難以認定\", \"難以採信\", \"不可採\"]),\n",
    "    ([\"矧\",], [\"況且\",]),\n",
    "    ([\"翻異\"], [\"推翻\"]),\n",
    "    ([\"乃\",], [\"於是\",]),\n",
    "    ([\"自白不諱\", \"供認不諱\", \"坦承不諱\"], [\"坦白承認\"]),\n",
    "    ([\"似無可採\", \"似屬無憑\", \"即無可採\", \"即屬無據\", \"尚無可採\", \"尚難憑採\", \"要非可信\", \"要屬虛言\", \"容非可採\"], [\"難以採信\", \"不可採信\", \"尚不足採信\", \"尚不足採證\"]),\n",
    "    ([\"顯有\",], [\"顯然有\", \"顯然屬於\", \"顯然是\"])\n",
    "]\n",
    "\n",
    "for legal_terms, usual_terms in term_lists:\n",
    "\n",
    "    terms = legal_terms+usual_terms\n",
    "    print(\"|\".join(terms))\n",
    "    save_path = os.path.join(\"llm_surprisals\", \"|\".join(terms))\n",
    "\n",
    "    if os.path.isfile(save_path): continue\n",
    "    term_types = [\"legal\"]*len(legal_terms)+[\"usual\"]*len(usual_terms)\n",
    "\n",
    "\n",
    "    regex_str = \"|\".join(terms)\n",
    "    idx = 0\n",
    "    contexts = {term: [] for term in terms}\n",
    "\n",
    "    context_sentences = []\n",
    "    term_list = []\n",
    "    for sentence1, sentence2 in zip(sentences[:-1], sentences[1:]):\n",
    "\n",
    "        matches = list(re.finditer(regex_str, sentence2))\n",
    "        if matches:\n",
    "            for match in matches:\n",
    "\n",
    "                context = sentence1+\"，\"+sentence2[:match.start()]\n",
    "                context_sentences.append(context)\n",
    "                term_list.append(match.group())\n",
    "\n",
    "    term_surprisals = {term: [] for term in terms}\n",
    "    for context_sentence, target_term in tqdm.tqdm([p for p in zip(context_sentences, term_list)]):\n",
    "        term_prob = 0\n",
    "        for term in set(terms):\n",
    "\n",
    "            term_prob+=get_word_probability(context_sentence, term, model, tokenizer)\n",
    "\n",
    "        term_surprisal = -np.log(term_prob)\n",
    "        term_surprisals[target_term].append(term_surprisal)\n",
    "\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump((term_surprisals, term_types), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
