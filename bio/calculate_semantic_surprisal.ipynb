{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1 = vec1.reshape(-1)\n",
    "    vec2 = vec2.reshape(-1)\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    # if similarity<0: similarity = 0\n",
    "    return similarity\n",
    "\n",
    "def similarity_to_probability(similarity):\n",
    "    \"\"\"Convert cosine similarity to a probability-like measure.\"\"\"\n",
    "    return ((1 + similarity) / 2)  # Rescale to [0,1]\n",
    "    return similarity\n",
    "\n",
    "def joint_probability_two(A, B):\n",
    "    \"\"\"Compute the joint probability of two vectors.\"\"\"\n",
    "    sim_AB = cosine_similarity(A, B)\n",
    "    return similarity_to_probability(sim_AB)\n",
    "\n",
    "def joint_probability_three(A, B, C):\n",
    "    \"\"\"Compute the joint probability of three vectors.\"\"\"\n",
    "    P_AB = joint_probability_two(A, B)\n",
    "    P_BC = joint_probability_two(B, C)\n",
    "    P_CA = joint_probability_two(C, A)\n",
    "    \n",
    "    return P_AB * P_BC * P_CA \n",
    "\n",
    "def joint_vector(A, B):\n",
    "    joint_vector = (A + B) / 2 * cosine_similarity(A, B)\n",
    "    \n",
    "    return joint_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "device = torch.device(\"mps\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embeddings(sentence, model, tokenizer):\n",
    "    tokens = tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    input_ids = tokens[\"input_ids\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.model.embed_tokens(input_ids).squeeze(0)\n",
    "    \n",
    "    embeddings = np.mean(embeddings.cpu().numpy(), axis=0)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, re\n",
    "\n",
    "sentences = []\n",
    "for f in glob.glob(\"PMC000xxxxxx/*.txt\"):\n",
    "    with open(f, \"r\", encoding=\"latin-1\") as file:\n",
    "        content = file.read()\n",
    "    for sentence in content.split(\"\\n\"):\n",
    "        if sentence.strip() and not sentence.startswith(\"==== \"):\n",
    "            sentence = sentence.strip()\n",
    "\n",
    "            sentences+=re.split(r\"[{. }!?;]\\s+\", sentence.replace(\"\\t\", \" \"))\n",
    "sentences = [s for s in sentences if len(s)>0]\n",
    "sentences = [s.lower() for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sentence_embeds = []\n",
    "\n",
    "for f in tqdm.tqdm(glob.glob(\"sentence_embeddings/*\")):\n",
    "    with open(f, \"rb\") as file:\n",
    "        selected_sentence_embeds+=[pickle.load(file)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_lists = [\n",
    "[[[\"bulla\", \"bullae\"]], [[\"blister\", \"blisters\"],]],\n",
    "[[[\"candida albicans\"], [\"candidiasis\"]], [[\"thrush\",]]],\n",
    "[[[\"carbohydrate\", \"carbohydrates\"]], [[\"carb\", \"carbs\"]]],\n",
    "[[[\"chemotherapy\", \"chemotherapies\"]], [[\"chemo\", \"chemoes\"]]],\n",
    "[[[\"chronic pain\", \"chronic pains\"]], [[\"persistent pain\", \"persistent pains\"]]],\n",
    "[[[\"comedo\", \"comedos\"]], [[\"whitehead\", \"whiteheads\"]]],\n",
    "[[[\"dermis\",], [\"epidermis\",]], [[\"skin\"]]],\n",
    "[[[\"dyspepsia\"]], [[\"indigestion\"]]],\n",
    "[[[\"erythrocyte\", \"erythrocytes\"]], [[\"red blood cell\", \"red blood cells\"]]],\n",
    "[[[\"febrile\"]], [[\"feverish\"]]],\n",
    "[[[\"haemorrhage\"]], [[\"heavy bleeding\"]]],\n",
    "[[[\"herpes zoster\"]], [[\"chickenpox\"]]],\n",
    "[[[\"hypertension\"]], [[\"high blood pressure\", \"high blood pressures\"]]],\n",
    "[[[\"hypotension\"]], [[\"low blood pressure\", \"low blood pressures\"]]],\n",
    "[[[\"influenza\"]], [[\"flu\"]]],\n",
    "[[[\"inhaler\"]], [[\"puffer\"]]],\n",
    "[[[\"intestine\"]], [[\"guts\"]]],\n",
    "[[[\"lethargy\"]], [[\"tiredness\"]]],\n",
    "[[[\"leukocyte\", \"leukocytes\"]], [[\"white blood cell\", \"white blood cells\"]]],\n",
    "[[[\"myocardial infarction\"]], [[\"heart attack\"]]],\n",
    "[[[\"pneumonia\"]], [[\"lung infection\"]]],\n",
    "[[[\"renal failure\"]], [[\"kidney failure\"]]],\n",
    "[[[\"thrombocytopenia\"]], [[\"low platelet count\", \"low platelet counts\"]]],\n",
    "[[[\"liposuction\"]], [[\"lipo\"]]],\n",
    "[[[\"melanoma\"]], [[\"skin cancer\"]]],\n",
    "]\n",
    "\n",
    "for legal_terms, usual_terms in term_lists:\n",
    "\n",
    "    terms = legal_terms+usual_terms\n",
    "    print(terms)\n",
    "\n",
    "    save_path = os.path.join(\"semantic_surprisals_v3\", \"|\".join([term[0] for term in terms]))\n",
    "\n",
    "    if os.path.isfile(save_path): continue\n",
    "    term_types = [\"legal\"]*len(legal_terms)+[\"usual\"]*len(usual_terms)\n",
    "\n",
    "\n",
    "    term_target_context_probs = {term[0]: [] for term in terms}\n",
    "    \n",
    "    for term in terms:\n",
    "\n",
    "        assert type(term)==list, \"Term must be a list\"\n",
    "        regex_str = fr'\\b(?:{\"|\".join(term)})\\b'\n",
    "        idx = 0\n",
    "\n",
    "        non_context_sentences = []\n",
    "        non_context_sentence_following_terms = []\n",
    "        context_sentences = []\n",
    "        term_list = []\n",
    "        for sentence1, sentence2 in zip(sentences[:-1], sentences[1:]):\n",
    "\n",
    "            matches = list(re.finditer(regex_str, sentence2))\n",
    "            if matches:\n",
    "                for match in matches:\n",
    "\n",
    "                    context = sentence1+\"; \"+sentence2[:match.start()]\n",
    "                    context_sentences.append(context)\n",
    "                    term_list.append(term[0])\n",
    "\n",
    "            else:\n",
    "                non_context_sentences+=[sentence1]\n",
    "                non_context_sentence_following_terms+=[sentence2.split(\" \")[0]]\n",
    "        non_context_embeds = []\n",
    "        non_context_following_term_embeds = []\n",
    "        for non_context_sentence in tqdm.tqdm(non_context_sentences, desc=\"Non-context sentences\"):\n",
    "            if non_context_sentence not in all_embeds:\n",
    "                all_embeds[non_context_sentence] = get_sentence_embeddings(non_context_sentence, model, tokenizer)\n",
    "            \n",
    "            non_context_embeds.append(all_embeds[non_context_sentence])\n",
    "\n",
    "        for non_context_sentence_following_term in tqdm.tqdm(non_context_sentence_following_terms, desc=\"Non-context sentence following terms\"):\n",
    "            if non_context_sentence_following_term not in all_embeds:\n",
    "                all_embeds[non_context_sentence_following_term] = get_sentence_embeddings(non_context_sentence_following_term, model, tokenizer)\n",
    "            \n",
    "            non_context_following_term_embeds.append(all_embeds[non_context_sentence_following_term])\n",
    "        \n",
    "        context_embeds = []\n",
    "        for context_sentence in tqdm.tqdm(context_sentences, desc=\"Context sentences\"):\n",
    "            if context_sentence not in all_embeds:\n",
    "                all_embeds[context_sentence] = get_sentence_embeddings(context_sentence, model, tokenizer)\n",
    "\n",
    "            context_embeds.append(all_embeds[context_sentence])\n",
    "\n",
    "        for t in term:\n",
    "            print(t)\n",
    "            if t not in all_embeds:\n",
    "                all_embeds[t] = get_sentence_embeddings(t, model, tokenizer)\n",
    "\n",
    "        pbar = tqdm.tqdm([(term, context_embed) for (term, context_embed) in zip(term_list, context_embeds)], desc=\"Counts\")\n",
    "        for this_term, target_context_embed in pbar:\n",
    "            target_context_counts = []\n",
    "            term_target_context_counts = []\n",
    "\n",
    "            for non_context_embed, non_context_following_term_embed in zip(non_context_embeds, non_context_following_term_embeds):\n",
    "                target_context_count = joint_probability_two(target_context_embed, non_context_embed)\n",
    "                target_context_counts.append(target_context_count)\n",
    "\n",
    "                term_counts = []\n",
    "                for t in term:\n",
    "                    t_embed = all_embeds[t]\n",
    "                    term_counts+=[joint_probability_two(non_context_following_term_embed, t_embed)]\n",
    "\n",
    "                term_count = sum(term_counts)\n",
    "\n",
    "                term_target_context_count = target_context_count * term_count\n",
    "                term_target_context_counts.append(term_target_context_count)\n",
    "                if target_context_count < term_target_context_count: raise ValueError\n",
    "\n",
    "            for context_embed in context_embeds:\n",
    "                target_context_count = joint_probability_two(target_context_embed, context_embed)\n",
    "                target_context_counts.append(target_context_count)\n",
    "\n",
    "                term_target_context_count = target_context_count * 1\n",
    "                term_target_context_counts.append(term_target_context_count)\n",
    "                if target_context_count < term_target_context_count: raise ValueError\n",
    "            \n",
    "            prob = sum(term_target_context_counts)/sum(target_context_counts)\n",
    "\n",
    "            term_target_context_probs[this_term].append(prob)\n",
    "\n",
    "            pbar.set_description(f\"Prob: {prob:.8f}\")\n",
    "\n",
    "    term_surprisals = {term[0]: [] for term in terms}\n",
    "\n",
    "    for term in term_target_context_probs:\n",
    "        for term_target_context_prob in term_target_context_probs[term]:\n",
    "            term_surprisal = -np.log(term_target_context_prob)\n",
    "            if term_surprisal < 0:\n",
    "                raise ValueError(\"Surprisal cannot be negative\")\n",
    "            term_surprisals[term].append(term_surprisal)\n",
    "\n",
    "    with open(save_path, \"wb\") as f:\n",
    "\n",
    "        pickle.dump((term_surprisals, term_types), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"overall_sentence_count.pickle\"):\n",
    "    with open(\"overall_sentence_count.pickle\", \"rb\") as f:\n",
    "        overall_sentence_count = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    all_sentence_counts = []\n",
    "    pbar = tqdm.tqdm(selected_sentence_embeds[:10000])\n",
    "    for target_sentence_embed in pbar:\n",
    "        target_sentence_count = 0\n",
    "        for context_sentence_embed in selected_sentence_embeds:\n",
    "            target_sentence_count+=joint_probability_two(target_sentence_embed, context_sentence_embed)\n",
    "\n",
    "        all_sentence_counts+=[target_sentence_count]\n",
    "\n",
    "    overall_sentence_count = np.mean(all_sentence_counts)*len(selected_sentence_embeds)\n",
    "    with open(\"overall_sentence_count.pickle\", \"wb\") as f:\n",
    "        pickle.dump(overall_sentence_count, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
