{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import glob\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "device = torch.device(\"mps\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_probability(context, word, model, tokenizer):\n",
    "    model.to(torch.device(\"mps\"))\n",
    "    # Tokenize the context and the word\n",
    "    context_tokens = tokenizer.encode(context, add_special_tokens=True)\n",
    "    word_tokens = tokenizer.encode(word, add_special_tokens=False)[1:]\n",
    "        \n",
    "    # Combine context with word tokens for probability calculation\n",
    "    combined_tokens = context_tokens + word_tokens\n",
    "        \n",
    "    total_prob = 1.0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(context_tokens), len(combined_tokens)):\n",
    "            # Get the input tokens up to the current token\n",
    "            input_tokens = combined_tokens[:i]\n",
    "            target_token = combined_tokens[i]\n",
    "                \n",
    "            # Convert to tensor\n",
    "            input_tensor = torch.tensor([input_tokens]).to(torch.device(\"mps\"))\n",
    "                \n",
    "            # Get model output and probabilities\n",
    "            outputs = model(input_tensor)\n",
    "            logits = outputs.logits  # (batch_size, seq_len, vocab_size)\n",
    "            last_token_logits = logits[0, -1, :]  # Get logits for the last token\n",
    "            probs = F.softmax(last_token_logits, dim=-1)\n",
    "\n",
    "            # Get the probability of the target token\n",
    "            token_prob = probs[target_token].item()\n",
    "            total_prob *= token_prob  # Multiply probabilities\n",
    "\n",
    "    return total_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, re\n",
    "\n",
    "sentences = []\n",
    "for f in glob.glob(\"PMC000xxxxxx/*.txt\"):\n",
    "    with open(f, \"r\", encoding=\"latin-1\") as file:\n",
    "        content = file.read()\n",
    "    for sentence in content.split(\"\\n\"):\n",
    "        if sentence.strip() and not sentence.startswith(\"==== \"):\n",
    "            sentence = sentence.strip()\n",
    "\n",
    "            sentences+=re.split(r\"[{. }!?;]\\s+\", sentence.replace(\"\\t\", \" \"))\n",
    "sentences = [s.lower() for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_lists = [\n",
    "[[[\"bulla\", \"bullae\"]], [[\"blister\", \"blisters\"],]],\n",
    "[[[\"candida albicans\"], [\"candidiasis\"]], [[\"thrush\",]]],\n",
    "[[[\"carbohydrate\", \"carbohydrates\"]], [[\"carb\", \"carbs\"]]],\n",
    "[[[\"chemotherapy\", \"chemotherapies\"]], [[\"chemo\", \"chemoes\"]]],\n",
    "[[[\"chronic pain\", \"chronic pains\"]], [[\"persistent pain\", \"persistent pains\"]]],\n",
    "[[[\"comedo\", \"comedos\"]], [[\"whitehead\", \"whiteheads\"]]],\n",
    "[[[\"dermis\",], [\"epidermis\",]], [[\"skin\"]]],\n",
    "[[[\"dyspepsia\"]], [[\"indigestion\"]]],\n",
    "[[[\"erythrocyte\", \"erythrocytes\"]], [[\"red blood cell\", \"red blood cells\"]]],\n",
    "[[[\"febrile\"]], [[\"feverish\"]]],\n",
    "[[[\"haemorrhage\"]], [[\"heavy bleeding\"]]],\n",
    "[[[\"herpes zoster\"]], [[\"chickenpox\"]]],\n",
    "[[[\"hypertension\"]], [[\"high blood pressure\", \"high blood pressures\"]]],\n",
    "[[[\"hypotension\"]], [[\"low blood pressure\", \"low blood pressures\"]]],\n",
    "[[[\"influenza\"]], [[\"flu\"]]],\n",
    "[[[\"inhaler\"]], [[\"puffer\"]]],\n",
    "[[[\"intestine\"]], [[\"guts\"]]],\n",
    "[[[\"lethargy\"]], [[\"tiredness\"]]],\n",
    "[[[\"leukocyte\", \"leukocytes\"]], [[\"white blood cell\", \"white blood cells\"]]],\n",
    "[[[\"myocardial infarction\"]], [[\"heart attack\"]]],\n",
    "[[[\"pneumonia\"]], [[\"lung infection\"]]],\n",
    "[[[\"renal failure\"]], [[\"kidney failure\"]]],\n",
    "[[[\"thrombocytopenia\"]], [[\"low platelet count\", \"low platelet counts\"]]],\n",
    "[[[\"liposuction\"]], [[\"lipo\"]]],\n",
    "[[[\"melanoma\"]], [[\"skin cancer\"]]],\n",
    "]\n",
    "\n",
    "for legal_terms, usual_terms in term_lists:\n",
    "\n",
    "    terms = legal_terms+usual_terms\n",
    "\n",
    "    save_path = os.path.join(\"llm_surprisals\", \"|\".join([term[0] for term in terms]))\n",
    "\n",
    "    if os.path.isfile(save_path): continue\n",
    "    term_types = [\"legal\"]*len(legal_terms)+[\"usual\"]*len(usual_terms)\n",
    "\n",
    "\n",
    "    term_surprisals = {term[0]: [] for term in terms}\n",
    "    for term in terms:\n",
    "        if type(term)!=list: raise ValueError(\"Term must be a list\")\n",
    "        regex_str = fr'\\b(?:{\"|\".join(term)})\\b'\n",
    "        idx = 0\n",
    "\n",
    "        context_sentences = []\n",
    "        for sentence1, sentence2 in zip(sentences[:-1], sentences[1:]):\n",
    "\n",
    "            matches = list(re.finditer(regex_str, sentence2))\n",
    "            if matches:\n",
    "                for match in matches:\n",
    "\n",
    "                    context = sentence1+\"; \"+sentence2[:match.start()]\n",
    "                    context_sentences.append(context)\n",
    "\n",
    "        \n",
    "        for context_sentence in tqdm.tqdm(context_sentences):\n",
    "            term_prob = 0\n",
    "            for t in term:\n",
    "\n",
    "                term_prob+=get_word_probability(context_sentence, t, model, tokenizer)\n",
    "\n",
    "            term_surprisal = -np.log(term_prob)\n",
    "            term_surprisals[term[0]].append(term_surprisal)\n",
    "\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump((term_surprisals, term_types), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
