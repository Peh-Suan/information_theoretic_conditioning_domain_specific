{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import re, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"term_type\": [],\n",
    "    \"term_group\": [],\n",
    "    \"term\": [],\n",
    "    \"surprisal\": [],\n",
    "    \"surprisal_type\": [],\n",
    "}\n",
    "for f in glob.glob(\"semantic_surprisals/*\"):\n",
    "    term_group = os.path.basename(f).replace(\".pickle\", \"\")\n",
    "    with open(f, \"rb\") as file:\n",
    "        term_surprisals, term_types = pickle.load(file)\n",
    "    \n",
    "    for term_idx, term in enumerate(term_surprisals):\n",
    "        for surprisal in term_surprisals[term]:\n",
    "            term_type = term_types[term_idx]\n",
    "            data[\"term_type\"].append(term_type)\n",
    "            data[\"term_group\"].append(term_group)\n",
    "            data[\"term\"].append(term)\n",
    "            data[\"surprisal\"].append(surprisal)\n",
    "            data[\"surprisal_type\"].append(\"semantic\")\n",
    "\n",
    "for f in glob.glob(\"llm_surprisals/*\"):\n",
    "    term_group = os.path.basename(f).replace(\".pickle\", \"\")\n",
    "    with open(f, \"rb\") as file:\n",
    "        term_surprisals, term_types = pickle.load(file)\n",
    "    \n",
    "    for term_idx, term in enumerate(term_surprisals):\n",
    "        for surprisal in term_surprisals[term]:\n",
    "            term_type = term_types[term_idx]\n",
    "            data[\"term_type\"].append(term_type)\n",
    "            data[\"term_group\"].append(term_group)\n",
    "            data[\"term\"].append(term)\n",
    "            data[\"surprisal\"].append(surprisal)\n",
    "            data[\"surprisal_type\"].append(\"llm\")\n",
    "\n",
    "for f in glob.glob(\"ngram_surprisals/*\"):\n",
    "    term_group = os.path.basename(f).replace(\".pickle\", \"\")\n",
    "    with open(f, \"rb\") as file:\n",
    "        term_surprisals, term_types = pickle.load(file)\n",
    "    \n",
    "    for term_idx, term in enumerate(term_surprisals):\n",
    "        for surprisal in term_surprisals[term]:\n",
    "            term_type = term_types[term_idx]\n",
    "            data[\"term_type\"].append(term_type)\n",
    "            data[\"term_group\"].append(term_group)\n",
    "            data[\"term\"].append(term)\n",
    "            data[\"surprisal\"].append(surprisal)\n",
    "            data[\"surprisal_type\"].append(\"ngram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, re\n",
    "\n",
    "sentences = []\n",
    "for f in glob.glob(\"PMC000xxxxxx/*.txt\"):\n",
    "    with open(f, \"r\", encoding=\"latin-1\") as file:\n",
    "        content = file.read()\n",
    "    for sentence in content.split(\"\\n\"):\n",
    "        if sentence.strip() and not sentence.startswith(\"==== \"):\n",
    "            sentence = sentence.strip()\n",
    "\n",
    "            sentences+=re.split(r\"[{. }!?;]\\s+\", sentence.replace(\"\\t\", \" \"))\n",
    "sentences = [s for s in sentences if len(s)>0]\n",
    "sentences = [s.lower() for s in sentences]\n",
    "\n",
    "term_lists = [\n",
    "[[[\"bulla\", \"bullae\"]], [[\"blister\", \"blisters\"],]],\n",
    "[[[\"candida albicans\", \"candidiasis\"]], [[\"thrush\",]]],\n",
    "[[[\"carbohydrate\", \"carbohydrates\"]], [[\"carb\", \"carbs\"]]],\n",
    "[[[\"chemotherapy\", \"chemotherapies\"]], [[\"chemo\", \"chemoes\"]]],\n",
    "[[[\"chronic pain\", \"chronic pains\"]], [[\"persistent pain\", \"persistent pains\"]]],\n",
    "[[[\"comedo\", \"comedos\"]], [[\"whitehead\", \"whiteheads\"]]],\n",
    "[[[\"dermis\",], [\"epidermis\",]], [[\"skin\"]]],\n",
    "[[[\"dyspepsia\"]], [[\"indigestion\"]]],\n",
    "[[[\"erythrocyte\", \"erythrocytes\"]], [[\"red blood cell\", \"red blood cells\"]]],\n",
    "[[[\"febrile\"]], [[\"feverish\"]]],\n",
    "[[[\"haemorrhage\"]], [[\"heavy bleeding\"]]],\n",
    "[[[\"herpes zoster\"]], [[\"chickenpox\"]]],\n",
    "[[[\"hypertension\"]], [[\"high blood pressure\", \"high blood pressures\"]]],\n",
    "[[[\"hypotension\"]], [[\"low blood pressure\", \"low blood pressures\"]]],\n",
    "[[[\"influenza\"]], [[\"flu\"]]],\n",
    "[[[\"inhaler\"]], [[\"puffer\"]]],\n",
    "[[[\"intestine\"]], [[\"guts\"]]],\n",
    "[[[\"lethargy\"]], [[\"tiredness\"]]],\n",
    "[[[\"leukocyte\", \"leukocytes\"]], [[\"white blood cell\", \"white blood cells\"]]],\n",
    "[[[\"myocardial infarction\"]], [[\"heart attack\"]]],\n",
    "[[[\"pneumonia\"]], [[\"lung infection\"]]],\n",
    "[[[\"renal failure\"]], [[\"kidney failure\"]]],\n",
    "[[[\"thrombocytopenia\"]], [[\"low platelet count\", \"low platelet counts\"]]],\n",
    "[[[\"liposuction\"]], [[\"lipo\"]]],\n",
    "[[[\"melanoma\"]], [[\"skin cancer\"]]],\n",
    "]\n",
    "\n",
    "\n",
    "term_list = []\n",
    "for legal_terms, usual_terms in tqdm.tqdm(term_lists):\n",
    "\n",
    "    terms = legal_terms+usual_terms\n",
    "    \n",
    "    for term in terms:\n",
    "        \n",
    "        if type(term)!=list: raise ValueError(\"Term must be a list\")\n",
    "        regex_str = fr'\\b(?:{\"|\".join(term)})\\b'\n",
    "\n",
    "        for sentence1, sentence2 in zip(sentences[:-1], sentences[1:]):\n",
    "\n",
    "            matches = list(re.finditer(regex_str, sentence2))\n",
    "            if matches:\n",
    "                for match in matches:\n",
    "\n",
    "                    term_list.append(term[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_semantic = pd.DataFrame(data)\n",
    "df_semantic = df_semantic[df_semantic[\"surprisal_type\"]==\"semantic\"].reset_index(drop=True)\n",
    "\n",
    "df_llm = pd.DataFrame(data)\n",
    "df_llm = df_llm[df_llm[\"surprisal_type\"]==\"llm\"].reset_index(drop=True)\n",
    "\n",
    "df_ngram = pd.DataFrame(data)\n",
    "df_ngram = df_ngram[df_ngram[\"surprisal_type\"]==\"ngram\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers_semantic = {}\n",
    "for term_group in set(df_semantic[\"term_group\"]):\n",
    "    scalers_semantic[term_group] = StandardScaler()\n",
    "    term_data = df_semantic[df_semantic[\"term_group\"] == term_group][\"surprisal\"].values.reshape(-1, 1)\n",
    "    scalers_semantic[term_group].fit(term_data)\n",
    "\n",
    "df_semantic[\"surprisal\"] = df_semantic.apply(\n",
    "    lambda row: scalers_semantic[row[\"term_group\"]].transform([[row[\"surprisal\"]]])[0][0], axis=1\n",
    ")\n",
    "\n",
    "scalers_llm = {}\n",
    "for term_group in set(df_llm[\"term_group\"]):\n",
    "    scalers_llm[term_group] = StandardScaler()\n",
    "    term_data = df_llm[df_llm[\"term_group\"] == term_group][\"surprisal\"].values.reshape(-1, 1)\n",
    "    scalers_llm[term_group].fit(term_data)\n",
    "df_llm[\"surprisal\"] = df_llm.apply(\n",
    "    lambda row: scalers_llm[row[\"term_group\"]].transform([[row[\"surprisal\"]]])[0][0], axis=1\n",
    ")\n",
    "scalers_ngram = {}\n",
    "for term_group in set(df_ngram[\"term_group\"]):\n",
    "    scalers_ngram[term_group] = StandardScaler()\n",
    "    term_data = df_ngram[df_ngram[\"term_group\"] == term_group][\"surprisal\"].values.reshape(-1, 1)\n",
    "    scalers_ngram[term_group].fit(term_data)\n",
    "df_ngram[\"surprisal\"] = df_ngram.apply(\n",
    "    lambda row: scalers_ngram[row[\"term_group\"]].transform([[row[\"surprisal\"]]])[0][0], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df_semantic, df_llm, df_ngram], ignore_index=True)\n",
    "df_combined.replace([float('inf'), float('-inf')], float('nan'), inplace=True)\n",
    "df_combined.dropna(inplace=True)\n",
    "df_combined[\"frequency\"] = df_combined[\"term\"].apply(lambda x: term_list.count(x))\n",
    "df_combined[\"term_type\"] = df_combined[\"term_type\"].apply(lambda x: x if x==\"usual\" else \"bio\")\n",
    "df_combined.to_csv(\"results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
